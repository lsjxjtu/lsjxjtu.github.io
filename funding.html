<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Sijia</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="links\CV_SLiu.pdf">CV(2&nbsp;pages)</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="funding.html" class="current">Projects</a></div>
<div class="menu-item"><a href="https://www.optml-group.com">OPTML&nbsp;Group</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="CSE891.html">CSE891</a></div>
<div class="menu-category">Others</div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Projects</h1>
</div>
<p>Trustworthy machine leanring (ML) and scalable ML are two main research directions investigated in my group. </p>
<h2>Trustworthy ML</h2>
<ul>
<li><p>Adversarial robustness of deep neural networks (DNNs)  </p>
</li>
<li><p>Deep model explanation </p>
</li>
<li><p>Fairness in ML</p>
</li>
<li><p>ML for security </p>
</li>
</ul>
<h2>Scalable ML</h2>
<ul>
<li><p>Zeroth-order learning for black-box optimization </p>
</li>
<li><p>Optimization theory and methods for deep learning (DL)</p>
</li>
<li><p>Deep model compression </p>
</li>
<li><p>DL in low-resource settings </p>
</li>
<li><p>Automated ML </p>
</li>
</ul>
<h1>Research highlights</h1>
<h2>Adversarial Robustness of Deep Neural Networks</h2>
<p><b>Description</b>: It has been widely known that deep neural networks (DNNs) are vulnerable to adversarial attacks that appear not only in the digital world but also in the physical world. <b>Along this direction, we highlight two of our achievements.</b></p>
<ul>
<li><p><b>First</b>, our ECCV’20 work designed 'Adversarial T-shirt&rsquo; [ <a href="https://arxiv.org/pdf/1910.11099.pdf">paper</a>, <a href="https://drive.google.com/file/d/1S9P56hdnQWC_Rffj1VQsHF-FcazQs2Xy/view?usp=sharing">demo</a>, over 200 media coverage on the web], a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person’s pose changes. We have shown that the adversarial T-shirt achieves 74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. </p>
</li>
</ul>
<table class="imgtable"><tr><td>
<img src="images/tshirt.png" alt="120" width="560px" />&nbsp;</td>
<td align="left"></td></tr></table>
<ul>
<li><p><b>Second</b>, our ICLR’21 work [<a href="https://openreview.net/pdf?id=PH5PH9ZO_4">paper</a>, <a href="https://github.com/ALFA-group/adversarial-code-generation">code</a>, <a href="https://news.mit.edu/2021/toward-deep-learning-models-that-can-reason-about-code-like-humans-0415">MIT-News</a>] designed &lsquo;Adversarial Program’, a method for finding and fixing weaknesses in automated programming tools. We have found that code-processing models can be deceived simply by renaming a variable, inserting a bogus print statement, or introducing other cosmetic operations into programs the model tries to process. These subtly altered programs function normally, but dupe the model into processing them incorrectly, rendering the wrong decision.  We show that our best attack proposal achieves a 52% improvement over a state-of-the-art attack generation approach for programs trained on a SEQ2SEQ model. We also show that the proposed adversarial programs can be further exploited to train a program languaging model robust against prediction-evasion attacks.</p>
</li>
<li><p><b>Selected Publications</b>:</p>
</li>
</ul>
<ol>
<li><p>S. Srikant, <b>S. Liu</b>, T. Mitrovska, S. Chang, Q. Fan, G. Zhang, U.-M. O'Reilly, <a href="https://arxiv.org/abs/2103.11882">Generating Adversarial Computer Programs using Optimized Obfuscations</a>, <i>ICLR’21</i></p>
</li>
<li><p>K. Xu, G. Zhang, <b>S. Liu</b>, Q. Fan, M. Sun, H. Chen, P.-Y. Chen, Y. Wang, X. Lin, <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500647.pdf">Adversarial T-shirt! Evading Person Detectors in A Physical World</a>, <i>ECCV’20</i> </p>
</li>
<li><p>R. Wang, G. Zhang, <b>S. Liu</b>, P.-Y. Chen, J. Xiong, M. Wang, <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680222.pdf">Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases</a>, <i>ECCV’20</i></p>
</li>
<li><p>A. Boopathy, <b>S. Liu</b>, G. Zhang, C. Liu, P.-Y. Chen, S. Chang, L. Daniel, <a href="http://proceedings.mlr.press/v119/boopathy20a.html">Proper Network Interpretability Helps Adversarial Robustness in Classification</a>, <i>ICML’20</i></p>
</li>
<li><p>T. Chen, <b>S. Liu</b>, S. Chang, Y. Cheng, L. Amini, Z. Wang, <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.pdf">Adversarial Robustness: From Self-Supervised Pretraining to Fine-Tuning</a>, <i>CVPR’20</i></p>
</li>
<li><p>K. Xu, H. Chen, <b>S. Liu</b>, P.-Y. Chen, T.-W. Wen, M. Hong, X. Lin, <a href="https://www.ijcai.org/Proceedings/2019/0550.pdf">Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective</a>, <i>IJCAI’19</i></p>
</li>
<li><p>K. Xu, <b>S. Liu</b>, P. Zhao, P.-Y. Chen, H. Zhang, D. Erdogmus, Y. Wang, X. Lin, <a href="https://arxiv.org/pdf/1808.01664.pdf">Structured Adversarial Attack: Towards General Implementation and Better Interpretability</a>, <i>ICLR’19</i></p>
</li>
</ol>
<h2>Zeroth-Order (ZO) Optimization: Theory, Methods and Applications [<a href="https://ieeexplore.ieee.org/document/9186148">Tutorial work</a> on IEEE Signal Processing Magazine]</h2>
<p><b>Description</b>: ZO optimization (learning without gradients) is increasingly embraced for solving many machine learning (ML) problems, where explicit expressions of the gradients are difficult or infeasible to obtain. Appealing applications include, e.g., robustness evaluation of black-box deep neural networks (DNNs), hyper-parameter optimization for automated ML, meta-learning, DNN diagnosis and explanation, and scientific discovery using black-box simulators. An illustrative example of ZO optimization versus first-order optimization is shown below.</p>
<table class="imgtable"><tr><td>
<img src="images/ZO.png" alt="90" width="420px" />&nbsp;</td>
<td align="left"></td></tr></table>
<p><b>Two of our achievements are highlighted below.</b></p>
<ul>
<li><p><b>First</b>, we have developed a series of theoretically-grounded ZO learning algorithms ranging from convex, nonconvex to min-max problems. </p>
</li>
<li><p><b>Second</b>, we have built a promising connection between ZO optimization and black-box poisoning and evasion attacks in the domain of adversarial ML. It has been shown that ZO optimization techniques can be used to mount a successful training-phase (poisoning) or testing-phase (evasion) attack in a fully black-box setting, where the adversary has no information about victim models and has to rely solely on the feedback stemming from the model input-output queries.</p>
</li>
<li><p><b>Selected Publications</b>:</p>
</li>
</ul>
<ol>
<li><p><b>S. Liu</b>, B. Kailkhura, P.-Y. Chen, P. Ting, S. Chang, L. Amini, <a href="https://papers.nips.cc/paper/2018/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf">Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization</a>, <i>NeurIPS’18</i></p>
</li>
<li><p>C.-C. Tu, P. Ting, P.-Y. Chen, <b>S. Liu</b>, H. Zhang, J. Yi, C.-J. Hsieh, S.-M. Cheng, <a href="https://arxiv.org/pdf/1805.11770v1.pdf">AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks</a>, <i>AAAI’19</i></p>
</li>
<li><p><b>S. Liu</b>, P.-Y. Chen, X. Chen, M. Hong, <a href="https://openreview.net/pdf?id=BJe-DsC5Fm">SignSGD via Zeroth-Order Oracle</a>, <i>ICLR’19</i></p>
</li>
<li><p>P. Zhao, <b>S. Liu</b>, P.-Y. Chen, N. Hoang, K. Xu, B. Kailkhura, X. Lin, <a href="https://par.nsf.gov/servlets/purl/10184578">On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method</a>, <i>ICCV’19</i></p>
</li>
<li><p>X. Chen, <b>S. Liu</b>, K. Xu, X. Li, X. Lin, M. Hong, D. Cox, <a href="https://arxiv.org/pdf/1910.06513.pdf">ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization</a>, <i>NeurIPS’19</i></p>
</li>
<li><p>M. Cheng, S. Singh, P.-Y. Chen, <b>S. Liu</b>, C.-J. Hsieh, <a href="https://arxiv.org/pdf/1909.10773.pdf">Sign-OPT: A Query-Efficient Hard-label Adversarial Attack</a>, <i>ICLR’20</i></p>
</li>
<li><p><b>S. Liu</b>, S. Lu, X. Chen, Y. Feng, K. Xu, A. Al-Dujaili, M. Hong, U.-M. O'Reilly, <a href="https://arxiv.org/pdf/1909.13806.pdf">Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML</a>, <i>ICML’20</i></p>
</li>
</ol>
<h1>Research Support</h1>
<p>I am very grateful to receive the support from <a href="https://mitibmwatsonailab.mit.edu">MIT-IBM Watson AI Lab</a>, <a href="https://beta.sam.gov/opp/f108cad02f824285af5ca85e1f7481f4/view">DARPA</a>, <a href="https://research.cisco.com">Cisco Research</a>, <a href="https://www.nsf.gov">NSF</a>, <a href="https://www.dso.org.sg">DSO</a>, <a href="https://www.llnl.gov">LLNL</a>, <a href="https://www.arl.army.mil/who-we-are/aro/">ARO</a>, <a href="https://safe.ai">CAIS (Center for AI safety)</a>, and <a href="https://www.schmidtsciences.org/">Schmidt Sciences</a>.</p>
<table class="imgtable"><tr><td>
<img src="images/support_logo_v1.png" alt="2121" width="792px" />&nbsp;</td>
<td align="left"></td></tr></table>
</td>
</tr>
</table>
</body>
</html>
