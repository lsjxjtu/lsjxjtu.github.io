<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Sijia Liu - CSE@MSU </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Sijia</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="links\CV_SLiu.pdf">CV(2&nbsp;pages)</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="funding.html">Projects</a></div>
<div class="menu-item"><a href="https://www.optml-group.com">OPTML&nbsp;Group</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="CSE891.html">CSE891</a></div>
<div class="menu-category">Others</div>
<div class="menu-item"><a href="talk.html">Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Sijia Liu - CSE@MSU </h1>
</div>
<table class="imgtable"><tr><td>
<img src="images/sijia.png" alt="Temp" width="130px" />&nbsp;</td>
<td align="left"><p>Associate Professor, <a href="https://www.cse.msu.edu/">Department of Computer Science and Engineering</a>,<br />
<a href="https://msu.edu/">Michigan State University</a>, East Lansing, MI 48824<br />
<a href="https://mitibmwatsonailab.mit.edu/people/sijia-liu/">Affiliated Professor@IBM Research, MIT-IBM Watson AI Lab affiliated PI</a>, Cambridge, MA 02142<br /> 

Email: liusiji5@msu.edu <br />
Twitter: @sijialiu17 <br />
<a href="https://scholar.google.com/citations?user=C7dO_UgAAAAJ&amp;hl=en">Google scholar</a> <br /></p>
</td></tr></table>
<div class="infoblock">
<div class="blocktitle">Prospective Students</div>
<div class="blockcontent">
<p>I always look for working with highly motivated students, in terms of RA/TA/externship/internship/visiting students.
Interested candidates are strongly encouraged to contact me by <a href="mailto:liusiji5@msu.edu">email</a>, together with resume and transcripts.</p>
</div></div>
<h2>Short Bio</h2>
<p>Sijia Liu is currently an Associate Professor at the CSE department of Michigan State University, an Affiliated Professor at IBM Research, and an MIT-IBM Watson AI Lab affiliated PI. He received the Ph.D. degree (with All-University Doctoral Prize) in Electrical and Computer Engineering from Syracuse University, NY, USA, in 2016. He was a Postdoctoral Research Fellow at the University of Michigan, Ann Arbor, in 2016-2017, and a Research Staff Member at the MIT-IBM Watson AI Lab in 2018-2020. His research interests include scalable and trustworthy AI, e.g. scalable optimization for deep models, machine unlearning for vision and language models, AI robustness, and data-model efficiency.</p>
<p>His accomplishments have also been recognized through numerous competitive awards. He received the NSF CAREER Award in 2024 for his work on zeroth-order optimization and was named to the AAAI New Faculty Highlights in 2023 for his contributions to scalable optimization for robust AI. In 2025, he was honored with the Withrow Rising Scholar Award (the highest recognition for junior faculty in MSU‚Äôs College of Engineering), and received the Aharon Katzir Young Investigator Award from the International Neural Network Society (INNS) for foundational contributions to robust and efficient neural networks. He has also received industry faculty awards such as the Amazon Research Award (2024) and the Cisco Research Awards (2022‚Äì2024). According to 2025 <a href="https://csrankings.org/">CSRankings</a>, he holds the #1 research productivity rank at MSU over the past decade, with a score of 89. His research has received several best paper honors, including the Best Paper Runner-Up Award at UAI 2022, IBM Pat Goldberg Best Paper Award Finalist (2020 and 2024), and the Best Student Paper Award at ICASSP 2017.</p>
<p>Dr. Liu is a Senior Member of IEEE and an active contributor to the research community. He serves on the IEEE Signal Processing Society‚Äôs Machine Learning for Signal Processing Technical Committee and as Associate Editor for both IEEE Transactions on Signal Processing and IEEE Transactions on Aerospace and Electronic Systems. He has served as Area Chair for ICASSP and ML conferences, including NeurIPS, ICML, ICLR, and AISTATS. He founded the New Frontiers in Adversarial Machine Learning Workshop series (ICML/NeurIPS 2021‚Äì2024) and was a Co-Chair of the Conference on Parsimony and Learning (CPAL) in 2024. In addition, he has delivered numerous tutorials on trustworthy and scalable ML at leading conferences, including ICASSP, AAAI, CVPR, and NeurIPS.</p>
<h2>Selected Awards</h2>
<ul>
<li><p>Aharon Katzir Young Investigator Award, International Neural Network Society (INNS), 2025</p>
</li>
<li><p>Withrow Rising Scholar Award, Michigan State University, 2025</p>
</li>
<li><p>Amazon Research Award, 2024</p>
</li>
<li><p>National Science Foundation (NSF) CAREER Award, 2024</p>
</li>
<li><p>Cisco Research Awards, 2022-2024</p>
</li>
<li><p>AAAI New Faculty Highlights, 2023</p>
</li>
<li><p>The Best Paper Runner-Up Award at 38th Conference on Uncertainty in Artificial Intelligence (UAI), 2022</p>
</li>
<li><p>IBM Pat Goldberg Best Paper Award Finalist, 2020</p>
</li>
<li><p>Best Student Paper Award at the 42nd International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017</p>
</li>
</ul>
<h2>Research Interests</h2>
<p>My research spans the areas of machine learning (ML)/deep learning (DL), optimization, computer vision, security, signal processing and data science, 
with a focus on developing learning algorithms and theory, and robust and explainable artificial intelligence (AI). 
These research themes provide a solid foundation for reaching my <b>long-term research objective: Making AI systems safe and scalable</b>. 
As AI moves from the lab into the real world (e.g., autonomous vehicles), ensuring its safety becomes a paramount requirement prior to 
its deployment. Moreover, as datasets, ML/DL models, and learning tasks become increasingly complex, 
getting ML/DL to scale calls for new advances in learning algorithm design.
Thus, robustness and scalability  underscore my current and future research, and of course, these two goals are intertwined.  
More  broadly,  the  study  towards robust and scalable AI could make a significant impact on machine learning theories, 
and induce more promising applications in, e.g., automated ML, meta-learning, privacy and security, hardware design, and big data analysis. 
I intend to seek a new learning frontier when the current learning algorithms become infeasible, and formalize  foundations  of  secure  learning.</p>
<p>Please refer to <a href="https://lsjxjtu.github.io/funding.html">Projects</a> and our <a href="https://www.optml-group.com/posts">OPTML</a> group for some research highlights.</p>
<p>üìñ For a more detailed introduction, see our 
<a href="/pdf/recruitment/Welcome2OPTML.pdf">Welcome2OPTML Booklet</a>.
</p>
<h2>Representative Publications</h2>
<ul>
<li><p><b>Trustworthy AI</b> : Robustness, Fairness, and Model Explanation </p>
</li>
</ul>
<ol>
<li><p><a href="https://www.nature.com/articles/s42256-025-00985-0">Rethinking Machine Unlearning For Large Language Models</a> <br />
<b>S. Liu</b>, Y. Yao*, J. Jia*, S. Casper, N. Baracaldo, P. Hase, Y. Yao, C. Y. Liu, X. Xu, H. Li, K. R. Varshney, M. Bansal, S. Koyejo, Y. Liu <br /> 
Nature Machine Intelligence, 2025, pp.181‚Äì194</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2310.11868">To Generate Or Not? Safety-driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images&hellip; For Now</a> <br />
Y. Zhang*, J. Jia*, X. Chen, A. Chen, Y. Zhang, J. Liu, K. Ding, and <b>S. Liu</b> <br /> 
ECCV‚Äô24</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2310.12508">Salun: Empowering Machine Unlearning Via Gradient-based Weight Saliency In Both Image Classification And Generation</a> <br />
C. Fan*, J. Liu*, Y. Zhang, E. Wong, D. Wei, <b>S. Liu</b> <br />
ICLR‚Äô24</p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.04934">Model Sparsity Can Simplify Machine Unlearning</a> <br /> 
J. Jia*, J. Liu*, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, <b>S. Liu</b> <br />
NeurIPS&rsquo;23</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.12376.pdf">Revisiting And Advancing Fast Adversarial Training Through The Lens Of Bi-level Optimization</a> <br /> 
Y. Zhang*, G. Zhang*, P. Khanduri, M. Hong, S. Chang, <b>S. Liu</b> (* Equal contribution) <br /> 
ICML‚Äô22</p>
</li>
</ol>
<ul>
<li><p><b>Scalable AI</b> : Model Compression, Distributed Learning, Black-box Optimization, and Automated ML <br /></p>
</li>
</ul>
<ol>
<li><p><a href="https://arxiv.org/pdf/2310.02025">Deepzero: Scaling Up Zeroth-order Optimization For Deep Model Training</a> <br />
A. Chen*, Y. Zhang*, J. Jia, J. Diffenderfer, J. Liu, K. Parasyris, Y. Zhang, Z. Zhang, B. Kailkhura, <b>S. Liu</b> <br /> 
ICLR‚Äô24</p>
</li>
<li><p><a href="https://arxiv.org/abs/2310.08782">Selectivity Drives Productivity: Efficient Dataset Pruning For Enhanced Transfer Learning</a> <br />
Y. Zhang*, Y. Zhang*, Aochuan Chen*, J. Jia, J. Liu, G. Liu, M. Hong, S. Chang, <b>S Liu</b> <br /> 
NeurIPS&rsquo;23</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2206.06257.pdf">Distributed Adversarial Training To Robustify Deep Neural Networks At Scale</a> <br />
G. Zhang*, S. Lu*, Y. Zhang, X. Chen, P.-Y. Chen, Q. Fan, L. Martie, L. Horesh, M. Hong, <b>S. Liu</b> <br />
UAI‚Äô22 (Best Paper Runner-Up Award)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2210.04092.pdf">Advancing Model Pruning Via Bi-level Optimization</a> <br />
Y. Zhang*, Y. Yao*, P. Ram, P. Zhao, T. Chen, M. Hong, Y. Wang, <b>S. Liu</b> <br />
NeurIPS‚Äô22</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1909.13806.pdf">Min-Max Optimization Without Gradients: Convergence And Applications To Adversarial ML</a> <br />
<b>S. Liu</b>, S. Lu, X. Chen, Y. Feng, K. Xu, A. Al-Dujaili, M. Hong, U.-M. O'Reilly <br />
ICML‚Äô20</p>
</li>
</ol>
<h2>News</h2>
<p>* We released the <a href="/pdf/recruitment/Welcome2OPTML.pdf">Welcome2OPTML Booklet</a> as a detailed introduction to our lab and an open invitation for strong prospective candidates to join us in 2026!</p>
<p>* Congrats to OPTML on three papers accepted at NeurIPS 2025! One spotlight and two posters, covering LLM unlearning, interpretability, and security on reasoning models!</p>
<p>* Congrats to Changsheng, Yihua, and Jinghan on their paper acceptance at the 18th ACM Workshop on Artificial Intelligence and Security (AISec).</p>
<p>* Congratulations to Changsheng, Chongyu, Yihua, and Jinghan on their paper, ‚Äú<a href="https://arxiv.org/pdf/2506.12963">Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills</a>‚Äù, which is accepted as a Main paper at EMNLP 2025.</p>
<p>* We are excited to announce that our tutorial ‚ÄúRobust Machine Unlearning: Securing Foundation Models Against Forgetting Failures‚Äù has been accepted for presentation at the IEEE Military Communications Conference (MILCOM 2025); see details at schedule [<a href="https://milcom2025.ieee-milcom.org/program/tutorials">link</a>].</p>
<p>* Honored to receive a new Medium Grant Award from the National Science Foundation (NSF) as the lead PI, in collaboration with PIs Dongxiao Zhu (Wayne State) and Sanmi Koyejo (Stanford). Grateful for NSF‚Äôs support and excited to advance our research together!</p>
<p>* Prof. Sijia Liu has been promoted to Associate Professor.</p>
<p>* Congratulations to Soumyadeep and Changsheng on their COLM‚Äô25 acceptance of ‚ÄúLLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks.‚Äù</p>
<p>* Congratulations to Yihua and Yuhao on their ICCV acceptance for their paper ‚ÄúInvisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design‚Äù.</p>
<p>* Excited and grateful to receive a gift award from the Center for AI Safety (CAIS).</p>
<p>* Dr. Liu is selected as the recipient of the <b>2024 Aharon Katzir Young Investigator Award</b> from the <a href="https://www.inns.org/">International Neural Network Society (INNS)</a>. </p>
<p>* Excited and grateful to receive a research grant from Open Philanthropy to support our AI safety research.</p>
<p>* <b>Two papers accepted at ICML 2025!</b> Congratulations to <b>Changsheng Wang</b> and <b>Chongyu Fan</b> on their first first-author ICML papers&ndash;well deserved!</p>
<p>* Grateful to participate in <a href="https://cse.buffalo.edu/~jsyuan/ResponsibleAI_NSF.html">the U.S.‚ÄìSoutheast Asia Regional Workshop on Responsible Artificial Intelligence</a>. An invaluable experience engaging in important discussions on advancing responsible AI globally.</p>
<p>* Congratulations to Yihua Zhang for winning the First-Place Award for the 2024-25 Fitch H. Beach Award!</p>
<p>* PI Liu is honored to receive the <a href="https://engineering.msu.edu/news-events/news/2025/02/20/msu-college-of-engineering-recognizes-18-for-excellence-in-contributions?utm_source=E-news&amp;utm_medium=newsletter&amp;utm_campaign=Q12025"><b>2025 Withrow Rising Scholar Award</b></a> at Michigan State University. This prestigious award annually recognizes junior faculty for excellence in instruction, scholarship, and distinguished service to the university and student body.</p>
<p>* Three papers in CVPR‚Äô25. Congrats to Yimeng Zhang and Yihua Zhang for their outstanding leadership!</p>
<p>* Congratulations to Yihua Zhang for being selected as the CSE department‚Äôs nominee for the prestigious <b>Fitch H. Beach Award</b> at Michigan State University! This distinguished award honors the most outstanding graduate researchers within the College of Engineering each year.</p>
<p>* ‚Äú<a href="https://www.nature.com/articles/s42256-025-00985-0">Rethinking machine unlearning for large language models</a>‚Äù is now published in Nature Machine Intelligence.</p>
<p>* Our ICLR‚Äô25 paper titled ‚Äú<a href="https://openreview.net/pdf?id=vRvVVb0NAz">When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</a>‚Äù is selected for an Oral (1.8% acceptance rate)</p>
<p>* Congratulations to Yihua Zhang for receiving <a href="https://research.ibm.com/university/awards/fellowships-awardees.html">the prestigious IBM PhD Fellowship</a> and <a href="https://cpal.cc/rising_stars_guidelines/">the CPAL Rising Star Award</a>.</p>
<p>* Congratulations to Brian Zhang for being named one of <a href="https://www.societyforscience.org/regeneron-sts/2025-scholars/">the Top 300 Scholars of the 84th Annual Science Talent Search in 2025</a> for his project <a href="https://ieeexplore.ieee.org/document/10447808">‚ÄúElevating Visual Prompting in Transfer Learning via Pruned Model Ensembles: No Retrain, No Pain‚Äù</a> conducted during his high school externship at OPTML mentored by Yuguang Yao.</p>
<p>* Honored to be one of the 10 recipients of the <a href="https://www.amazon.science/research-awards/program-updates/10-amazon-research-awards-recipients-announced?utm_campaign=10-amazon-research-awards-recipients-announced&amp;utm_medium=organic-asw&amp;utm_source=linkedin&amp;utm_content=2024-12-20-10-amazon-research-awards-recipients-announced&amp;utm_term=2024-dec">Amazon Research Award</a> for Spring and Winter 2024.</p>
<p>* Check out the <a href="https://www.optml-group.com/images/pubpic/neurips2024_flyer.pdf">OPTML Menu of Innovations @ NeurIPS 2024</a>!</p>
<p>* Thrilled to announce that our position paper, ‚Äú<a href="https://arxiv.org/abs/2402.08787">Rethinking Machine Unlearning for LLMs</a>‚Äù, has been accepted for publication in <a href="https://www.nature.com/natmachintell/">Nature Machine Intelligence</a>.  Congratulations to the team and our amazing collaborators for achieving this milestone!</p>
<p>* Grateful for receiving the NAIRR Pilot Award in the field of Artificial Intelligence and Intelligent Systems.</p>
<p>* One paper in <a href="https://wacv2025.thecvf.com">WACV‚Äô25</a>: <a href="https://arxiv.org/pdf/2303.07474">Can Adversarial Examples Be Parsed to Reveal Victim Model Information?</a></p>
<p>* Six papers in <a href="https://neurips.cc">NeurIPS‚Äô24</a>, including one dataset benchmark. Congrats to Yihua Zhang, Yuguang Yao, Jinghan Jia, and Yimeng Zhang for their outstanding leadership!</p>
<p>* One paper in <a href="https://2024.emnlp.org">EMNLP‚Äô24</a>: <a href="https://arxiv.org/abs/2404.18239">SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning</a>.</p>
<p>* Grateful to receive the Amazon Research Award for AI in Information Security&ndash;Spring 2024.</p>
<p>* <a href="https://advml-frontier.github.io/">The 3rd AdvML-Frontiers Workshop</a> is now live and will be co-located at NeurIPS&rsquo;24! Submit your papers by Aug 30.</p>
<p>* Dr. Liu has received the prestigious NSF Faculty Early Career Development (CAREER) Award.</p>
<p>* Congratulations to Yihua for receiving the <a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars/">2024 MLCommons Rising Stars Award</a>.</p>
<p>* Two papers in <a href="https://eccv.ecva.net/">ECCV‚Äô24</a>: (1) <a href="https://arxiv.org/pdf/2310.11868">Exploring adversarial robustness of safety-driven concept-unlearned diffusion models through a diffusion classifier perspective</a>; (2) <a href="https://arxiv.org/pdf/2403.07362">Challenging forgets to unveil when and why machine unlearning could be more challenging than common beliefs</a>.</p>
<p>* Two papers in <a href="https://icml.cc/">ICML‚Äô24</a>: (1) <a href="https://www.arxiv.org/pdf/2402.11592">Benchmarking zeroth-order optimization for memory-efficient LLM fine-tuning</a>; (2) <a href="https://arxiv.org/pdf/2406.01977">Why does graph transformer generalize? A Theoretical Dive into Self-attention and Positional Encoding</a>.</p>
<p>* [Feature Article@IEEE SPM] We are thrilled to share that our tutorial artile titled <a href="https://ieeexplore.ieee.org/document/10502023">‚ÄúAn Introduction to Bilevel Optimization: Foundations and applications in signal processing and machine learning‚Äù</a> has been published in the IEEE Signal Processing Magazine as a Feature Article.</p>
<p>* [New Preprints] We are pleased to announce the release of the following papers on arXiv:<br /> [1] <a href="https://arxiv.org/abs/2402.08787">Rethinking Machine Unlearning for Large Language Models</a>;<br />  [2] <a href="https://arxiv.org/abs/2402.11592">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</a>; <br /> [3] <a href="https://arxiv.org/abs/2402.11846">UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models</a>; <br /> [4] <a href="https://arxiv.org/abs/2403.07362">Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning</a>; <br /> [5] <a href="https://arxiv.org/abs/2404.18239">SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning</a>; <br /> [6] <a href="https://arxiv.org/abs/2405.15234">Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models</a>.</p>
<p>* We are thrilled to share that our research paper titled <a href="https://www.nowpublishers.com/article/Details/SEC-039">‚ÄúReverse Engineering Deceptions in Machine- and Human-Centric Attacks‚Äù</a> has been officially published in Foundations and Trends¬Æ in Privacy and Security.</p>
<p>* [Launch of the MSU-UM-ARO <a href="https://project-aro.netlify.app/">Project Website</a>] The ‚ÄúLifelong Multimodal Fusion by Cross Layer Distributed Optimization‚Äù project receives funding from the Army Research Office (ARO). </p>
<p>* Tutorial ‚ÄúMachine Unlearning in Computer Vision: Foundations and Applications‚Äù is accepted for presentation by CVPR 2024. See you in Seattle!</p>
<p>* <i>Four</i> papers in <a href="https://iclr.cc/">ICLR‚Äô24</a>: (1) <a href="https://arxiv.org/pdf/2310.12508.pdf">Machine unlearning for safe image generation</a>; (2) <a href="https://arxiv.org/pdf/2310.02025.pdf">DeepZero: Training neural networks from scratch using only forward passes</a>; (3) Backdoor data sifting; (4) <a href="https://arxiv.org/pdf/2310.08381.pdf">Visual prompting automation</a></p>
<p>* [New Preprints] We are pleased to announce the release of the following papers on arXiv:<br /> [1] <a href="https://arxiv.org/abs/2310.11868">To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images &hellip; For Now</a>;<br />  [2] <a href="https://arxiv.org/pdf/2311.02373v1.pdf">From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models</a>.</p>
<p>* Tutorial on ‚ÄúZeroth-Order Machine Learning: Fundamental Principles and Emerging Applications in Foundation Models‚Äù is accepted by ICASSP‚Äô24 and AAAI‚Äô24.</p>
<p>* NeurIPS 2023: 3 Papers Accepted ‚Äì 1 Spotlight and 2 Posters. Congratulations to Jignhan, Jiancheng, and Yuguang for their spotlight acceptance with 'Model Sparsity Simplifies Machine Unlearning.&rsquo; And kudos to Yihua, Yimeng, Aochuan, Jinghan, and Jiancheng for their poster acceptance with 'Selectivity Boosts Transfer Learning Efficiency.&rsquo;</p>
<p>* Grateful to receive a grant from Army Research Office (ARO) as the PI.</p>
<p>* Our paper on <a href="https://arxiv.org/abs/2308.10110v1">Adversarial Training for MoE</a> has been chosen for an Oral Presentation at <a href="https://iccv2023.thecvf.com">ICCV&rsquo;23</a>.</p>
<p>* Grateful to receive a gift funding from Cisco Research as the PI.</p>
<p>* Call for participation in <a href="https://advml-frontier.github.io">2nd AdvML-Frontiers Workshop</a>@ICML&rsquo;23.</p>
<p>* <i>One</i> paper in <a href="https://iccv2023.thecvf.com">ICCV&rsquo;23</a> on Adversarial Robustness of Mixture-of-Experts.</p>
<p>* Grateful to receive a CPS Medium Grant Award from NSF as a co-PI.</p>
<p>* Slides of our CVPR&rsquo;23 tutorial on Reverse Engineering of Deceptions (RED) is now available at the tutorial page. <a href="https://sites.google.com/view/cvpr2023red">[link]</a></p>
<p>* Our paper &ldquo;<a href="https://arxiv.org/pdf/2210.06284.pdf">Visual Prompting for Adversarial Robustness</a>&rdquo; received the <a href="https://drive.google.com/file/d/1PHjGXkCUSRJ3pQc4_8BGl_Y5aaRB-Nuw/view">Top 3% Paper Recognition</a> at ICASSP 2023. Congrats to Aochuan, Peter (internship at OPTML in 2022), Yuguang, and Pin-Yu (IBM Research)!</p>
<p>* Grateful to be elected as Associate Editor of IEEE Transactions on Aerospace and Electronic Systems.</p>
<p>* <i>Two</i> papers in <a href="https://icml.cc/">ICML&rsquo;23</a> and CFP for <a href="https://advml-frontier.github.io/">2nd AdvML-Frontiers Workshop</a>.</p>
<p>* A new arXiv paper is released: Model Sparsification Can Simplify Machine Unlearning! <a href="https://arxiv.org/pdf/2304.04934.pdf">[Paper]</a> <a href="https://github.com/OPTML-Group/Unlearn-Sparse">[Code]</a></p>
<p>* Grateful to receive a grant from Lawrence Livermore National Laboratory.</p>
<p>* Call for Papers and AdvML Rising Star Award Applications in the workshop <a href="https://advml-frontier.github.io/">AdvML-Frontiers</a>, ICML&rsquo;23</p>
<p>* A new arXiv paper is released: Adversarial attacks can be parsed to reveal victim model information! <a href="https://arxiv.org/pdf/2303.07474.pdf">[Paper]</a></p>
<p>* The 2nd Workshop on <a href="https://advml-frontier.github.io/">New Frontiers in Adversarial Machine Learning</a> has been accepted by <b>ICML&rsquo;23</b>.</p>
<p>* Grateful to receive a grant from DSO National Laboratories.</p>
<p>* <i>Two</i> papers in <a href="https://cvpr2023.thecvf.com">CVPR&rsquo;23</a>.</p>
<p>* <i>Three</i> papers in <a href="https://2023.ieeeicassp.org">ICASSP&rsquo;23</a>.</p>
<p>* CVPR&rsquo;23 tutorial on <i>Reverse Engineering of Deception: Foundations and Applications</i> is accepted and will be given with Xiaoming Liu (MSU) and Xue Lin (Northeastern).</p>
<p>* AAAI&rsquo;23 tutorial on Bi-level Optimization in ML:  Foundations and Applications is now available at <a href="https://sites.google.com/view/aaai2023tutorial/home">link</a>.</p>
<p>* <i>Four</i> papers in <a href="https://iclr.cc/Conferences/2023/Dates">ICLR&rsquo;23</a>: <a href="https://openreview.net/pdf?id=MjsDeTcDEy">Issues and Fixes in IRM</a>, <a href="https://openreview.net/pdf?id=5tKXUZil3X">TextGrad: Differentiable Solution to NLP Attack Generation</a>, <a href="https://openreview.net/pdf?id=4UldFtZ_CVF">Provable Benefits of Sparse GNN</a>, <a href="https://openreview.net/pdf?id=jClGv3Qjhb">Sample Complexity Analysis of ViT</a>.</p>
<p>* <i>One</i> paper in <a href="https://www.aspdac.com/aspdac2023/">ASP-DAC&rsquo;23</a>.</p>
<p>* <i>One</i> paper in <a href="https://saner2023.must.edu.mo/">SANER 2023</a>: <a href="https://arxiv.org/abs/2211.11711">Towards Both Robust and Accurate Code Models</a>; Equally contributed by Jinghan Jia (MSU) and Shashank Srikant (MIT).</p>
<p>* Grateful to be selected as a presenter of the AAAI 2023 New Faculty Highlight Program.</p>
<p>* Tutorial on <a href="https://sites.google.com/view/neurips2022-frfm-turotial/">Foundational Robustness of Foundation Models</a> will be given in NeurIPS&rsquo;22.</p>
<p>* Tutorial on Bi-level Machine Learning will be given in <a href="https://aaai.org/Conferences/AAAI-23/aaai23tutorials/">AAAI&rsquo;23</a>.</p>
<p>* <i>Two</i> papers in <a href="https://nips.cc">NeurIPS&rsquo;22</a>.</p>
<p>* Grateful to receive a Robust Intelligence (RI) Core Small Grant Award from NSF as the PI.</p>
<p>* Grateful to receive the Best Paper Runner-Up Award at UAI‚Äô2022 in recognition of our work ‚Äú<a href="https://arxiv.org/pdf/2206.06257.pdf">Distributed Adversarial Training to Robustify Deep Neural Networks at Scale</a>‚Äù.</p>
<p>* <i>One</i> paper in <a href="https://www.auai.org/uai2022/">UAI‚Äô22</a> (Oral presentation).</p>
<p>* <i>Five</i> papers in <a href="https://icml.cc/Conferences/2022/CallForPapers">ICML‚Äô22</a>: <a href="https://arxiv.org/pdf/2112.12376.pdf">Bi-level adversarial training</a>; <a href="https://arxiv.org/pdf/2206.04762.pdf">Winning lottery tickets from robust pretraining</a>; <a href="https://arxiv.org/pdf/2206.07839v1.pdf">Pruning helps certified robustness</a>; <a href="https://arxiv.org/pdf/2112.04468.pdf">Contrastive learning theory</a>; Generalization theory of GCN.</p>
<p>* <i>One</i> paper in <a href="https://2022.naacl.org">NAACL&rsquo;22</a>.</p>
<p>* <i>One</i> paper in <a href="https://ijcai-22.org/">IJCAI&rsquo;22</a>.</p>
<p>* CFP: 1st Workshop on New Frontiers in Adversarial Machine Learning at ICML&rsquo;22 (<a href="https://advml-frontier.github.io/">AdvML-Frontiers@ICML&rsquo;22</a>).</p>
<p>* Grateful to receive a gift funding from Cisco Research as the PI.</p>
<p>* Congratulations to Yihua Zhang for his first CVPR paper.</p>
<p>* <i>Two</i> papers in <a href="https://cvpr2022.thecvf.com">CVPR 2022</a>.</p>
<p>* Congratulations to Yimeng Zhang, Yuguang Yao, Jianghan Jia for their first ICLR papers.</p>
<p>* <i>Five</i> papers in <a href="https://iclr.cc/">ICLR 2022</a>: <a href="https://openreview.net/pdf?id=gpp7cf0xdfN">Reverse Engineering of Adversaries</a>, <a href="https://openreview.net/pdf?id=W9G_ImpHlQd">Black-Box Defense</a> (spotlight), <a href="https://openreview.net/pdf?id=VqzXzA9hjaX">Learning to Optimize</a>, <a href="https://openreview.net/pdf?id=qiMXBIf4NfB">Self-Training Theory</a>, <a href="https://openreview.net/pdf?id=oj2yn1Q4Ett">Distributed Learning</a>.</p>
<p>* Our work on <a href="https://arxiv.org/pdf/2112.12376.pdf"><i>interpreting and advancing adversarial training via bi-level optimization</i></a> is now available on arXiv; equally contributed by Yihua Zhang (MSU) and Guanhua Zhang (UCSB).</p>
<p>* Grateful to receive a DARPA IP2 AIE Grant as a Co-PI.</p>
<p>* <i>Five</i> papers in <a href="https://nips.cc">NeurIPS 2021</a>.</p>
<p>* Our MSU-NEU team (with PI <a href="https://www.cse.msu.edu/~liuxm/index2.html">Xiaoming Liu</a> and co-PI <a href="https://web.northeastern.edu/xuelin/?page_id=60">Xue Lin</a>) entered the Phase 2 of DARPA AIE RED.</p>
<p>* <i>One</i> paper in <a href="https://icml.cc">ICML 2021</a>.</p>
<p>* MIT news ‚Äò<a href="https://news.mit.edu/2021/toward-deep-learning-models-that-can-reason-about-code-like-humans-0415">Toward deep-learning models that can reason about code more like humans</a>‚Äô on our ICLR‚Äô21 work ‚ÄòAdversarial Programs‚Äô [[https:<i></i>openreview.net/pdf?id=P</p>
<div id="footer">
<div id="footer-text">
Page generated 2025-09-18 15:44:42 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
